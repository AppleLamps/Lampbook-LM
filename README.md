<div align="center">
<img width="1200" height="475" alt="GHBanner" src="https://github.com/user-attachments/assets/0aa67016-6eaf-458a-adb2-6e31a0763ed6" />
</div>

# LampbookLM - AI-Powered Research Assistant

A powerful research assistant with RAG (Retrieval-Augmented Generation) capabilities, conversational memory, and streaming responses. Built with React, TypeScript, and Google's Gemini AI.

View the original app in AI Studio: https://ai.studio/apps/drive/1L9feqW929e_NuJ2vjJ0S6g9UAzKYPlgR

## ğŸš€ Latest Features

### 1. **Backend URL Fetching** ğŸŒ
- âœ… Bypasses CORS restrictions with server-side fetching
- âœ… Uses Mozilla Readability for clean content extraction
- âœ… Filters out ads, navbars, and footers automatically

### 2. **Conversational Memory** ğŸ’­
- âœ… Full multi-turn conversation support
- âœ… Context-aware follow-up questions ("Why?", "Tell me more")
- âœ… Maintains chat history throughout the session

### 3. **RAG Implementation** ğŸ”
- âœ… Document chunking with smart boundary detection
- âœ… Vector embeddings using Gemini Embeddings API
- âœ… Semantic search for relevant content retrieval
- âœ… Scalable to hundreds of documents

### 4. **Enhanced UX** âœ¨
- âœ… **Toast Notifications**: Non-blocking notifications instead of alerts
- âœ… **Streaming Responses**: Real-time text generation with typing effect
- âœ… **Stop Generation Button**: Cancel ongoing AI responses
- âœ… **Smart Clear Options**: Separate "Clear Chat" and "Clear Workspace"

## ğŸ“‹ Prerequisites

- **Node.js** 18+ and npm
- **Google Gemini API key** - [Get one here](https://ai.google.dev/)

## ğŸ› ï¸ Installation

### 1. Install Dependencies

```bash
# Frontend dependencies
npm install

# Backend dependencies
cd server
npm install
cd ..
```

### 2. Configure Environment

Create a `.env` file in the root directory:

```env
GEMINI_API_KEY=your_api_key_here
VITE_API_URL=http://localhost:3001
```

Or use `.env.local` (both work).

## ğŸƒ Run Locally

### Option 1: Run Both Servers Together (Recommended)

```bash
npm run dev:all
```

This starts both backend and frontend in a single terminal! ğŸ‰

### Option 2: Run Separately

**Terminal 1 - Backend Server:**
```bash
cd server
npm run dev
```
âœ… Backend starts on `http://localhost:3001`

**Terminal 2 - Frontend:**
```bash
npm run dev
```
âœ… Frontend starts on `http://localhost:3000`

---

Visit **`http://localhost:3000`** in your browser.

## ğŸ“š How to Use

### Adding Sources
1. **Upload Files**: Click "Add Source Files" to upload .txt or .pdf files
2. **Add URLs**: Paste a URL and click "Add" to fetch web content
3. **View Sources**: Click on any source to preview its content

### Chatting
1. Type your question in the chat input
2. Press `Enter` to send (or `Shift+Enter` for new line)
3. Watch as the AI streams its response in real-time
4. Click **[1]** citations to jump to the source document

### Managing Your Workspace
- **Eye Icon**: Exclude/include sources from analysis
- **X Icon**: Delete a source
- **Clear Chat**: Remove conversation history (keeps sources)
- **Clear Workspace**: Remove everything (sources + chat)

### Synthesis Tools
- **Synthesize Summary**: Get a comprehensive overview
- **Create Outline**: See structured key points
- **Make Flashcards**: Generate Q&A study cards

## ğŸ—ï¸ Architecture

```
LampbookLM/
â”œâ”€â”€ server/                    # Express backend
â”‚   â”œâ”€â”€ index.js              # URL fetching with Readability
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ services/                 # Frontend services
â”‚   â”œâ”€â”€ geminiService.ts     # AI chat & streaming
â”‚   â”œâ”€â”€ ragService.ts        # RAG pipeline
â”‚   â””â”€â”€ documentParser.ts    # File processing
â”œâ”€â”€ hooks/
â”‚   â””â”€â”€ useLampbook.ts       # Main state management
â”œâ”€â”€ components/              # React components
â”‚   â”œâ”€â”€ ChatPanel.tsx        # Chat interface
â”‚   â”œâ”€â”€ SourcePanel.tsx      # Source management
â”‚   â””â”€â”€ icons/
â””â”€â”€ App.tsx                  # Main app component
```

## ğŸ§  How RAG Works

### 1. Document Ingestion
- Documents are chunked into 1000-character segments with 200-char overlap
- Each chunk is embedded using `text-embedding-004`
- Embeddings are stored in an in-memory vector database

### 2. Query Processing
- Your question is embedded into a vector
- Top 5 most similar chunks are retrieved using cosine similarity
- Only relevant excerpts are sent to Gemini (not entire documents!)

### 3. Response Generation
- Gemini generates streaming responses based on relevant chunks
- Citations are automatically added
- Conversation history is maintained for follow-ups

## ğŸ”§ Technical Stack

- **Frontend**: React 19, TypeScript, Vite, Tailwind CSS
- **Backend**: Express, Node.js
- **AI**: Google Gemini 2.5 Flash
- **Key Libraries**:
  - `@google/genai` - Gemini SDK
  - `@mozilla/readability` - Content extraction
  - `react-hot-toast` - Notifications
  - `jsdom` - HTML parsing

## ğŸš¨ Troubleshooting

### Backend won't start
- Check that port 3001 is available
- Verify `GEMINI_API_KEY` is set in `.env`

### URL fetching fails
- Ensure backend is running
- Check URL is publicly accessible
- Some sites block automated scraping

### Slow responses
- First query generates embeddings (one-time cost per document)
- Large documents take longer to chunk
- Consider excluding unused sources

### Streaming not working
- Check browser console for errors
- Verify Gemini API key is valid
- Try refreshing the page

## ğŸ“¦ Build for Production

```bash
# Frontend
npm run build

# Backend
cd server
npm start
```

## ğŸ”® Future Enhancements

- [ ] Persistent vector storage (IndexedDB or backend DB)
- [ ] Support for DOCX, HTML, Markdown
- [ ] Export chat history
- [ ] Custom chunk size configuration
- [ ] Multiple embedding models

## ğŸ“„ License

MIT

---

**Note**: This application requires a valid Google Gemini API key. API usage is subject to Google's pricing and rate limits.
